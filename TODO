Overall TODO
============
- How to manage names between scripts?
  - Naming scheme must be hierarchal & stable
    e.g. "/memcached/logs/latest"
  - Directly on top of file-system?
  - Want to be able define new ad-hoc collections of data
- Overnight jobs (using barley?)
- Compare data across runs in spite of different addresses
  - Disbale ASLR?
    - What about mmap/malloc calls?
  - Investigate variability between different runs
    - Take average of page counts across several runs?
- Define higher-level tasks (simple bash or python scripts?)
- Wrapper around python matplotlib
- Retain thread-level traces (i.e. don't do thread => process aggregation in pin module)
- Add timing information to trace
- Store command(-line arguments) that generated trace
- User-level named plots for different runs (similar to user-level named collections?)
- Run-time configurable logging for pin module

Initial ideas for primitives
============================

Users interact with system in two ways:
- Specify workload through CLI
- Analyze multiple runs (usually to create plots)

CLI primitives:
- Run
  - Job that creates one trace of memcached, parsec, etc.
  - Defined by command-line options
  - Can be run locally or on barley
  - Can belong to at most one collection
- Collection
  - Named group of runs
  - Can define matchers on command-line options for runs within a collection (but not across collections?)

Analysis primitives:
- Trace
  - Output of one run
  - Internally composed of data from multiple threads
  - Tracks reads/writes to each page
- Trace aggregation
  - Be able to aggregate/slice across different page numbers, cache (enabled/disabled), threads
